Use CPU
>>>>>>>start training setting: Exchange_336_96_DLinear_custom_ftM_sl336_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>
train set 4880
val set 665
test set 1422
	iters: 100, epoch: 1 | loss: 0.8583100
	speed: 0.1766s/iter; estimate training left time: 1059.6570s
	iters: 200, epoch: 1 | loss: 0.5893295
	speed: 0.0020s/iter; estimate training left time: 11.8839s
	iters: 300, epoch: 1 | loss: 0.3423858
	speed: 0.0020s/iter; estimate training left time: 11.6674s
	iters: 400, epoch: 1 | loss: 0.5392980
	speed: 0.0020s/iter; estimate training left time: 11.5737s
	iters: 500, epoch: 1 | loss: 0.9105919
	speed: 0.0018s/iter; estimate training left time: 10.2124s
	iters: 600, epoch: 1 | loss: 0.8304930
	speed: 0.0018s/iter; estimate training left time: 9.9886s
Epoch: 1 cost time: 19.072038888931274
Epoch: 1, Steps: 610 | Train Loss: 0.5654185 Vali Loss: 0.2865492 Test Loss: 0.2414628
Validation Set loss decreased (inf --> 0.286549).              Saving model checkpoint.pth...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.7197754
	speed: 0.4989s/iter; estimate training left time: 2689.5347s
	iters: 200, epoch: 2 | loss: 0.3411064
	speed: 0.0020s/iter; estimate training left time: 10.6890s
	iters: 300, epoch: 2 | loss: 0.5015829
	speed: 0.0022s/iter; estimate training left time: 11.2515s
	iters: 400, epoch: 2 | loss: 0.9668908
	speed: 0.0018s/iter; estimate training left time: 9.3477s
	iters: 500, epoch: 2 | loss: 0.6078534
	speed: 0.0019s/iter; estimate training left time: 9.6974s
	iters: 600, epoch: 2 | loss: 0.5804117
	speed: 0.0019s/iter; estimate training left time: 9.5258s
Epoch: 2 cost time: 17.63107919692993
Epoch: 2, Steps: 610 | Train Loss: 0.5527122 Vali Loss: 0.8856776 Test Loss: 0.5523790
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00025
	iters: 100, epoch: 3 | loss: 0.2696839
	speed: 0.5026s/iter; estimate training left time: 2402.7248s
	iters: 200, epoch: 3 | loss: 0.7213504
	speed: 0.0019s/iter; estimate training left time: 8.9981s
	iters: 300, epoch: 3 | loss: 0.5696129
	speed: 0.0019s/iter; estimate training left time: 8.9305s
	iters: 400, epoch: 3 | loss: 0.4151638
	speed: 0.0019s/iter; estimate training left time: 8.5736s
	iters: 500, epoch: 3 | loss: 0.3391891
	speed: 0.0020s/iter; estimate training left time: 8.8250s
	iters: 600, epoch: 3 | loss: 0.7910196
	speed: 0.0018s/iter; estimate training left time: 7.5649s
Epoch: 3 cost time: 17.745418310165405
Epoch: 3, Steps: 610 | Train Loss: 0.5488535 Vali Loss: 0.6851990 Test Loss: 0.4458458
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.000125
	iters: 100, epoch: 4 | loss: 0.6578416
	speed: 0.4922s/iter; estimate training left time: 2053.1627s
	iters: 200, epoch: 4 | loss: 0.6356599
	speed: 0.0017s/iter; estimate training left time: 6.8861s
	iters: 300, epoch: 4 | loss: 0.5192603
	speed: 0.0017s/iter; estimate training left time: 6.8204s
	iters: 400, epoch: 4 | loss: 0.7864556
	speed: 0.0017s/iter; estimate training left time: 6.7502s
	iters: 500, epoch: 4 | loss: 0.4707579
	speed: 0.0017s/iter; estimate training left time: 6.4317s
	iters: 600, epoch: 4 | loss: 0.5725865
	speed: 0.0017s/iter; estimate training left time: 6.3521s
Epoch: 4 cost time: 17.091986179351807
Epoch: 4, Steps: 610 | Train Loss: 0.5468279 Vali Loss: 0.6387576 Test Loss: 0.4142542
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing setting: Exchange_336_96_DLinear_custom_ftM_sl336_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<
test set 1422
预测值和真实值之间的误差:mse:0.241462841629982, mae:0.33927032351493835, rse:0.3737964928150177
预测值和真实值之间的相关系数:corr:[0.24146701 0.24160211 0.24043396 0.240257   0.23978752 0.23987484
 0.24053673 0.23971084 0.23870653 0.23939441 0.23850062 0.23980804
 0.24014135 0.23808149 0.23893669 0.23970519 0.23967235 0.23816124
 0.23970908 0.23945987 0.23994292 0.23879708 0.23856735 0.23815593
 0.23768507 0.2404219  0.23830847 0.24044453 0.23841065 0.23890948
 0.23854461 0.24111906 0.2397308  0.23914243 0.2404281  0.24064422
 0.23929831 0.24083035 0.23907818 0.23787037 0.24016637 0.24006462
 0.2406701  0.24230844 0.24090628 0.23884651 0.24207275 0.24119334
 0.23980157 0.24224639 0.24102601 0.23893695 0.2388551  0.2412863
 0.24062116 0.24099636 0.24009871 0.23936546 0.23998752 0.23891483
 0.2393804  0.24030569 0.24256699 0.23946217 0.23936142 0.23971237
 0.24002048 0.23885968 0.24369949 0.24099307 0.24201867 0.23843025
 0.23759033 0.24179773 0.24013792 0.23978537 0.23880908 0.24112438
 0.24231115 0.24067806 0.24208225 0.2370723  0.2404981  0.24123733
 0.24249943 0.2387802  0.24308093 0.24200046 0.23924443 0.24162862
 0.24206014 0.24051772 0.24046925 0.23975223 0.23783508 0.23795238]
