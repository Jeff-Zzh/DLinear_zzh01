Use CPU
>>>>>>>start training setting: Exchange_336_96_DLinear_custom_ftM_sl336_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>
train 4880
val 665
test 1422
	iters: 100, epoch: 1 | loss: 0.2270500
	speed: 0.2091s/iter; estimate training left time: 1254.8157s
	iters: 200, epoch: 1 | loss: 0.0933224
	speed: 0.0024s/iter; estimate training left time: 14.1207s
	iters: 300, epoch: 1 | loss: 0.0717278
	speed: 0.0025s/iter; estimate training left time: 14.7641s
	iters: 400, epoch: 1 | loss: 0.1070747
	speed: 0.0025s/iter; estimate training left time: 14.2668s
	iters: 500, epoch: 1 | loss: 0.1255078
	speed: 0.0023s/iter; estimate training left time: 13.0444s
	iters: 600, epoch: 1 | loss: 0.2555734
	speed: 0.0024s/iter; estimate training left time: 13.1634s
Epoch: 1 cost time: 22.713727474212646
Epoch: 1, Steps: 610 | Train Loss: 0.1483462 Vali Loss: 0.1342610 Test Loss: 0.0997939
Validation Set loss decreased (inf --> 0.134261).              Saving model checkpoint.pth...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.1007526
	speed: 0.6241s/iter; estimate training left time: 3364.3926s
	iters: 200, epoch: 2 | loss: 0.0681183
	speed: 0.0024s/iter; estimate training left time: 12.8638s
	iters: 300, epoch: 2 | loss: 0.1112817
	speed: 0.0025s/iter; estimate training left time: 13.0599s
	iters: 400, epoch: 2 | loss: 0.1152925
	speed: 0.0025s/iter; estimate training left time: 12.7739s
	iters: 500, epoch: 2 | loss: 0.1481426
	speed: 0.0024s/iter; estimate training left time: 12.0822s
	iters: 600, epoch: 2 | loss: 0.0856659
	speed: 0.0024s/iter; estimate training left time: 11.9792s
Epoch: 2 cost time: 22.107855319976807
Epoch: 2, Steps: 610 | Train Loss: 0.1249588 Vali Loss: 0.2311258 Test Loss: 0.1129564
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00025
	iters: 100, epoch: 3 | loss: 0.0902897
	speed: 0.6203s/iter; estimate training left time: 2965.5096s
	iters: 200, epoch: 3 | loss: 0.1077121
	speed: 0.0025s/iter; estimate training left time: 11.7169s
	iters: 300, epoch: 3 | loss: 0.1272599
	speed: 0.0031s/iter; estimate training left time: 14.3572s
	iters: 400, epoch: 3 | loss: 0.0741545
	speed: 0.0025s/iter; estimate training left time: 11.4089s
	iters: 500, epoch: 3 | loss: 0.2661076
	speed: 0.0027s/iter; estimate training left time: 11.6659s
	iters: 600, epoch: 3 | loss: 0.0596090
	speed: 0.0026s/iter; estimate training left time: 11.2447s
Epoch: 3 cost time: 22.179056406021118
Epoch: 3, Steps: 610 | Train Loss: 0.1188835 Vali Loss: 0.1350880 Test Loss: 0.0821026
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.000125
	iters: 100, epoch: 4 | loss: 0.1978399
	speed: 0.6371s/iter; estimate training left time: 2657.2677s
	iters: 200, epoch: 4 | loss: 0.2046631
	speed: 0.0027s/iter; estimate training left time: 10.9984s
	iters: 300, epoch: 4 | loss: 0.1206582
	speed: 0.0024s/iter; estimate training left time: 9.5108s
	iters: 400, epoch: 4 | loss: 0.1052557
	speed: 0.0025s/iter; estimate training left time: 9.6511s
	iters: 500, epoch: 4 | loss: 0.2686453
	speed: 0.0024s/iter; estimate training left time: 9.1172s
	iters: 600, epoch: 4 | loss: 0.0727324
	speed: 0.0023s/iter; estimate training left time: 8.5541s
Epoch: 4 cost time: 22.869389057159424
Epoch: 4, Steps: 610 | Train Loss: 0.1165242 Vali Loss: 0.2274227 Test Loss: 0.1087510
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing setting: Exchange_336_96_DLinear_custom_ftM_sl336_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<
test 1422
mse:0.09979385882616043, mae:0.22087323665618896, rse:0.24030454456806183, corr:[0.2565454  0.25635704 0.2561616  0.25595495 0.255838   0.25552878
 0.25536925 0.2551008  0.25483987 0.2546277  0.25457656 0.25447184
 0.25463757 0.25483263 0.2547283  0.2548185  0.25483233 0.2549039
 0.25486183 0.25481755 0.25480148 0.2547266  0.25471666 0.254756
 0.25489062 0.2549289  0.2548251  0.25483274 0.25479034 0.25477028
 0.25478953 0.2549631  0.255      0.25499457 0.25491476 0.25480998
 0.25482103 0.25474712 0.25455508 0.254514   0.25450447 0.25442904
 0.25435188 0.2541834  0.25405625 0.25416967 0.2542083  0.25424752
 0.25417164 0.25426945 0.25438643 0.25446856 0.25450262 0.25458714
 0.2546154  0.25475308 0.25489956 0.25494793 0.25507966 0.25504607
 0.2550986  0.25513756 0.2551734  0.25511807 0.255054   0.2550167
 0.2550819  0.25509363 0.2551044  0.25506043 0.25514862 0.25514784
 0.25520357 0.2552188  0.25528023 0.2552925  0.25526145 0.25533387
 0.25529185 0.2553608  0.25538415 0.25538933 0.25554198 0.25562668
 0.25563496 0.25568846 0.2557166  0.2557059  0.2557219  0.25576425
 0.25580806 0.25578907 0.2557256  0.25569895 0.2556594  0.25561202]
